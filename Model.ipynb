{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anubhavyadav111/Movie-Review-Model-Using-RNN/blob/main/Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJ0ZKrDxffoG"
      },
      "source": [
        "#**Importing Necessary libraries**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7iXDJSIGjjn-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd    # to load dataset\n",
        "import numpy as np     # for mathematic equation\n",
        "from nltk.corpus import stopwords   # to get collection of stopwords\n",
        "from sklearn.model_selection import train_test_split       # for splitting dataset\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer  # to encode text to int\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences   # to do padding or truncating\n",
        "from tensorflow.keras.models import Sequential     # the model\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense # layers of the architecture\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint   # save model\n",
        "from tensorflow.keras.models import load_model   # load saved model\n",
        "import re\n",
        "from tensorflow.keras.layers import SimpleRNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzRVuD3qkGvD"
      },
      "source": [
        "# **Preparing the data named IMDB**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpIe4GYX0dx8"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RI03JfP7kEKA",
        "outputId": "ff518128-f9b4-4898-c3ad-f02872185498"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                  review sentiment\n",
            "0      One of the other reviewers has mentioned that ...  positive\n",
            "1      A wonderful little production. <br /><br />The...  positive\n",
            "2      I thought this was a wonderful way to spend ti...  positive\n",
            "3      Basically there's a family where a little boy ...  negative\n",
            "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
            "...                                                  ...       ...\n",
            "49995  I thought this movie did a down right good job...  positive\n",
            "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
            "49997  I am a Catholic taught in parochial elementary...  negative\n",
            "49998  I'm going to have to disagree with the previou...  negative\n",
            "49999  No one expects the Star Trek movies to be high...  negative\n",
            "\n",
            "[50000 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "data = pd.read_csv('IMDB Dataset.csv', encoding=\"latin-1\"\n",
        "                  )\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4KcdMAKtQan"
      },
      "source": [
        "Stop Word is a commonly used words in a sentence, usually a search engine is programmed to ignore this words (i.e. \"the\", \"a\", \"an\", \"of\", etc.)\n",
        "Declaring the english stop words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjJrIijipG5D",
        "outputId": "4c497275-7d6e-4be6-fc35-6a6b5ddf4626"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "english_stops = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlWrgDKttZC1"
      },
      "source": [
        "# **Load and Clean Dataset**\n",
        "**In the original dataset, the reviews are still dirty. There are still html tags, numbers, uppercase, and punctuations. This will not be good for training, so in load_dataset() function, beside loading the dataset using pandas, I also pre-process the reviews by removing html tags, non alphabet (punctuations and numbers), stop words, and lower case all of the reviews.**\n",
        "\n",
        "# **Encode Sentiments**\n",
        "**In the same function, We also encode the sentiments into integers (0 and 1). Where 0 is for negative sentiments and 1 is for positive sentiments.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NM_5RjUNqa-s",
        "outputId": "cadb1923-2012-4739-cd7a-48b172d9d264"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reviews\n",
            "0        [one, reviewers, mentioned, watching, oz, epis...\n",
            "1        [a, wonderful, little, production, the, filmin...\n",
            "2        [i, thought, wonderful, way, spend, time, hot,...\n",
            "3        [basically, family, little, boy, jake, thinks,...\n",
            "4        [petter, mattei, love, time, money, visually, ...\n",
            "                               ...                        \n",
            "49995    [i, thought, movie, right, good, job, it, crea...\n",
            "49996    [bad, plot, bad, dialogue, bad, acting, idioti...\n",
            "49997    [i, catholic, taught, parochial, elementary, s...\n",
            "49998    [i, going, disagree, previous, comment, side, ...\n",
            "49999    [no, one, expects, star, trek, movies, high, a...\n",
            "Name: review, Length: 50000, dtype: object \n",
            "\n",
            "Sentiment\n",
            "0        1\n",
            "1        1\n",
            "2        1\n",
            "3        0\n",
            "4        1\n",
            "        ..\n",
            "49995    1\n",
            "49996    0\n",
            "49997    0\n",
            "49998    0\n",
            "49999    0\n",
            "Name: sentiment, Length: 50000, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "def load_dataset():\n",
        "    df = pd.read_csv('IMDB Dataset.csv',  encoding='latin-1')\n",
        "    x_data = df['review']       # Reviews/Input\n",
        "    y_data = df['sentiment']    # Sentiment/Output\n",
        "\n",
        "    # PRE-PROCESS REVIEW\n",
        "    x_data = x_data.replace({'<.*?>': ''}, regex = True)          # remove html tag\n",
        "    x_data = x_data.replace({'[^A-Za-z]': ' '}, regex = True)     # remove non alphabet\n",
        "    x_data = x_data.apply(lambda review: [w for w in review.split() if w not in english_stops])  # remove stop words\n",
        "    x_data = x_data.apply(lambda review: [w.lower() for w in review])   # lower case\n",
        "\n",
        "    # ENCODE SENTIMENT -> 0 & 1\n",
        "    y_data = y_data.replace('positive', 1)\n",
        "    y_data = y_data.replace('negative', 0)\n",
        "\n",
        "    return x_data, y_data\n",
        "\n",
        "x_data, y_data = load_dataset()\n",
        "\n",
        "print('Reviews')\n",
        "print(x_data, '\\n')\n",
        "print('Sentiment')\n",
        "print(y_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcQbFcH0thD1"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "#**Split Dataset**\n",
        "**In this work, We decided to split the data into 80% of Training and 20% of Testing set using train_test_split method from Scikit-Learn. By using this method, it automatically shuffles the dataset. We need to shuffle the data because in the original dataset, the reviews and sentiments are in order, where they list positive reviews first and then negative reviews. By shuffling the data, it will be distributed equally in the model, so it will be more accurate for predictions.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cy2sU3jthtz",
        "outputId": "de5ba3c6-6be1-4203-a75d-fcd863c8029d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Set\n",
            "37892    [this, truly, hilarious, film, one, i, seen, m...\n",
            "39081    [i, especially, liked, ending, movie, i, reall...\n",
            "9615     [this, great, movie, in, genre, memphis, belle...\n",
            "28472    [there, something, one, characters, aging, fil...\n",
            "16707    [the, main, complaint, film, fact, i, can, not...\n",
            "                               ...                        \n",
            "28660    [julie, andrews, rock, hudson, great, movie, m...\n",
            "7282     [i, really, like, show, as, part, greek, life,...\n",
            "1896     [this, movie, offers, nothing, anyone, it, suc...\n",
            "48493    [distasteful, cliched, thriller, young, couple...\n",
            "21320    [yes, absolutely, dreadful, and, coming, someo...\n",
            "Name: review, Length: 40000, dtype: object \n",
            "\n",
            "23950    [jack, kate, meet, physician, daniel, farady, ...\n",
            "3965     [this, wes, craven, worst, worst, horror, call...\n",
            "34102    [spoilers, sex, huh, it, one, basic, parts, hu...\n",
            "19971    [david, duchovny, plays, lead, role, film, now...\n",
            "48034    [yes, i, sentimental, schmaltzy, but, movie, t...\n",
            "                               ...                        \n",
            "24569    [one, worst, romantic, comedies, nay, worst, m...\n",
            "2404     [i, think, movie, rated, correctly, i, took, c...\n",
            "14261    [this, espionage, melodrama, nice, almost, pro...\n",
            "4451     [there, good, movies, bad, movies, moscow, zer...\n",
            "28786    [a, early, oliver, stone, associate, produced,...\n",
            "Name: review, Length: 10000, dtype: object \n",
            "\n",
            "Test Set\n",
            "37892    1\n",
            "39081    1\n",
            "9615     1\n",
            "28472    1\n",
            "16707    0\n",
            "        ..\n",
            "28660    1\n",
            "7282     1\n",
            "1896     0\n",
            "48493    0\n",
            "21320    0\n",
            "Name: sentiment, Length: 40000, dtype: int64 \n",
            "\n",
            "23950    1\n",
            "3965     0\n",
            "34102    1\n",
            "19971    1\n",
            "48034    1\n",
            "        ..\n",
            "24569    0\n",
            "2404     0\n",
            "14261    0\n",
            "4451     0\n",
            "28786    1\n",
            "Name: sentiment, Length: 10000, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size = 0.2)\n",
        "\n",
        "print('Train Set')\n",
        "print(x_train, '\\n')\n",
        "print(x_test, '\\n')\n",
        "print('Test Set')\n",
        "print(y_train, '\\n')\n",
        "print(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B712-ID4tnDY"
      },
      "source": [
        "**Function for getting the average review length, by calculating the mean of all the reviews length (using numpy.mean)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5n86Ria5toOI"
      },
      "outputs": [],
      "source": [
        "def get_max_length():\n",
        "    review_length = []\n",
        "    for review in x_train:\n",
        "        review_length.append(len(review))\n",
        "\n",
        "    return int(np.ceil(np.mean(review_length)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwYgTbAEts_G"
      },
      "source": [
        "#**Tokenize and Pad/Truncate Reviews**\n",
        "**A Neural Network only accepts numeric data, so we need to encode the reviews. I use tensorflow.keras.preprocessing.text.Tokenizer to encode the reviews into integers, where each unique word is automatically indexed (using fit_on_texts method) based on x_train.**\n",
        "\n",
        "**x_train and x_test is converted into integers using texts_to_sequences method.**\n",
        "\n",
        "**Each reviews has a different length, so we need to add padding (by adding 0) or truncating the words to the same length (in this case, it is the mean of all reviews length) using tensorflow.keras.preprocessing.sequence.pad_sequences.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cRXNvFHttoe",
        "outputId": "33d25289-0eb0-4637-9184-29d1afc19ef8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Words: 92215\n",
            "Encoded X Train\n",
            " [[   8  279  481 ...    0    0    0]\n",
            " [   1  167  330 ...    0    0    0]\n",
            " [   8   21    3 ...    0    0    0]\n",
            " ...\n",
            " [   8    3 1450 ...    0    0    0]\n",
            " [9331 6826  600 ...    0    0    0]\n",
            " [ 322  331 1967 ...    0    0    0]] \n",
            "\n",
            "Encoded X Test\n",
            " [[ 556 2163  812 ...    0    0    0]\n",
            " [   8 4906 4907 ...    0    0    0]\n",
            " [ 948  287 3763 ...  577 2224  178]\n",
            " ...\n",
            " [   8 7848 2411 ...    0    0    0]\n",
            " [  50    9   28 ... 7044  381   93]\n",
            " [  39  308 2598 ...    4  192   81]] \n",
            "\n",
            "Maximum review length:  130\n"
          ]
        }
      ],
      "source": [
        "# ENCODE REVIEW\n",
        "token = Tokenizer(lower=False)    # no need lower, because already lowered the data in load_data()\n",
        "token.fit_on_texts(x_train)\n",
        "x_train = token.texts_to_sequences(x_train)\n",
        "x_test = token.texts_to_sequences(x_test)\n",
        "\n",
        "max_length = get_max_length()\n",
        "\n",
        "x_train = pad_sequences(x_train, maxlen=max_length, padding='post', truncating='post')\n",
        "x_test = pad_sequences(x_test, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "total_words = len(token.word_index) + 1   # add 1 because of 0 padding\n",
        "print('Total Words:', total_words)\n",
        "\n",
        "print('Encoded X Train\\n', x_train, '\\n')\n",
        "print('Encoded X Test\\n', x_test, '\\n')\n",
        "print('Maximum review length: ', max_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXf1daJctzRK"
      },
      "source": [
        "#**Build Architecture/Model**\n",
        "**Embedding Layer: in simple terms, it creates word vectors of each word in the word_index and group words that are related or have similar meaning by analyzing other words around them.**\n",
        "\n",
        "**RNN Layer: to make a decision to keep or throw away data by considering the current input, previous output.**\n",
        "\n",
        "**Dense Layer: compute the input with the weight matrix and bias (optional), and using an activation function. I use Sigmoid activation function for this work because the output is only 0 or 1.**\n",
        "\n",
        "**The optimizer is Adam and the loss function is Binary Crossentropy because again the output is only 0 and 1, which is a binary number.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoM7MNAKtz50",
        "outputId": "ffea77ba-7d83-4417-d2db-73f51025df5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 130, 32)           2950880   \n",
            "_________________________________________________________________\n",
            "simple_rnn (SimpleRNN)       (None, 64)                6208      \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 2,957,153\n",
            "Trainable params: 2,957,153\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "rnn = Sequential()\n",
        "\n",
        "rnn.add(Embedding(total_words,32,input_length =max_length))\n",
        "rnn.add(SimpleRNN(64,input_shape = (total_words, max_length), return_sequences=False,activation=\"relu\"))\n",
        "rnn.add(Dense(1, activation = 'sigmoid')) #flatten\n",
        "\n",
        "print(rnn.summary())\n",
        "rnn.compile(loss=\"binary_crossentropy\",optimizer='adam',metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0krGNkMm6TPV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqi0Y67xFeyZ"
      },
      "source": [
        "#**Training the Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xiygrqGN6TwI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udaR9DEFu6Fr",
        "outputId": "beaebcd4-0fb7-4526-e031-cae5f50a6d62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train on 40000 samples\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-08-25 04:48:02.339942: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
            "2023-08-25 04:48:02.508103: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1669] Found device 0 with properties: \n",
            "name: NVIDIA A100-SXM4-40GB major: 8 minor: 0 memoryClockRate(GHz): 1.41\n",
            "pciBusID: 0000:07:00.0\n",
            "2023-08-25 04:48:02.509435: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1669] Found device 1 with properties: \n",
            "name: NVIDIA A100-SXM4-40GB major: 8 minor: 0 memoryClockRate(GHz): 1.41\n",
            "pciBusID: 0000:0f:00.0\n",
            "2023-08-25 04:48:02.510767: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1669] Found device 2 with properties: \n",
            "name: NVIDIA A100-SXM4-40GB major: 8 minor: 0 memoryClockRate(GHz): 1.41\n",
            "pciBusID: 0000:47:00.0\n",
            "2023-08-25 04:48:02.512076: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1669] Found device 3 with properties: \n",
            "name: NVIDIA A100-SXM4-40GB major: 8 minor: 0 memoryClockRate(GHz): 1.41\n",
            "pciBusID: 0000:4e:00.0\n",
            "2023-08-25 04:48:02.513674: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1669] Found device 4 with properties: \n",
            "name: NVIDIA A100-SXM4-40GB major: 8 minor: 0 memoryClockRate(GHz): 1.41\n",
            "pciBusID: 0000:87:00.0\n",
            "2023-08-25 04:48:02.514997: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1669] Found device 5 with properties: \n",
            "name: NVIDIA A100-SXM4-40GB major: 8 minor: 0 memoryClockRate(GHz): 1.41\n",
            "pciBusID: 0000:90:00.0\n",
            "2023-08-25 04:48:02.516325: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1669] Found device 6 with properties: \n",
            "name: NVIDIA A100-SXM4-40GB major: 8 minor: 0 memoryClockRate(GHz): 1.41\n",
            "pciBusID: 0000:b7:00.0\n",
            "2023-08-25 04:48:02.517639: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1669] Found device 7 with properties: \n",
            "name: NVIDIA A100-SXM4-40GB major: 8 minor: 0 memoryClockRate(GHz): 1.41\n",
            "pciBusID: 0000:bd:00.0\n",
            "2023-08-25 04:48:02.517669: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "2023-08-25 04:48:02.536184: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
            "2023-08-25 04:48:02.539322: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
            "2023-08-25 04:48:02.539570: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
            "2023-08-25 04:48:02.540053: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11\n",
            "2023-08-25 04:48:02.540856: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
            "2023-08-25 04:48:02.541000: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
            "2023-08-25 04:48:02.559118: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1797] Adding visible gpu devices: 0, 1, 2, 3, 4, 5, 6, 7\n",
            "2023-08-25 04:48:02.581033: I tensorflow/core/platform/profile_utils/cpu_utils.cc:109] CPU Frequency: 2245680000 Hz\n",
            "2023-08-25 04:48:02.601576: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0xb4e4080 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2023-08-25 04:48:02.601630: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2023-08-25 04:48:03.545549: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0xd75a390 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2023-08-25 04:48:03.545594: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A100-SXM4-40GB, Compute Capability 8.0\n",
            "2023-08-25 04:48:03.545600: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): NVIDIA A100-SXM4-40GB, Compute Capability 8.0\n",
            "2023-08-25 04:48:03.545605: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (2): NVIDIA A100-SXM4-40GB, Compute Capability 8.0\n",
            "2023-08-25 04:48:03.545611: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (3): NVIDIA A100-SXM4-40GB, Compute Capability 8.0\n",
            "2023-08-25 04:48:03.545616: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (4): NVIDIA A100-SXM4-40GB, Compute Capability 8.0\n",
            "2023-08-25 04:48:03.545622: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (5): NVIDIA A100-SXM4-40GB, Compute Capability 8.0\n",
            "2023-08-25 04:48:03.545627: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (6): NVIDIA A100-SXM4-40GB, Compute Capability 8.0\n",
            "2023-08-25 04:48:03.545633: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (7): NVIDIA A100-SXM4-40GB, Compute Capability 8.0\n",
            "2023-08-25 04:48:03.586300: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1669] Found device 0 with properties: \n",
            "name: NVIDIA A100-SXM4-40GB major: 8 minor: 0 memoryClockRate(GHz): 1.41\n",
            "pciBusID: 0000:07:00.0\n",
            "2023-08-25 04:48:03.589236: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1669] Found device 1 with properties: \n",
            "name: NVIDIA A100-SXM4-40GB major: 8 minor: 0 memoryClockRate(GHz): 1.41\n",
            "pciBusID: 0000:0f:00.0\n",
            "2023-08-25 04:48:03.590493: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1669] Found device 2 with properties: \n",
            "name: NVIDIA A100-SXM4-40GB major: 8 minor: 0 memoryClockRate(GHz): 1.41\n",
            "pciBusID: 0000:47:00.0\n",
            "2023-08-25 04:48:03.591743: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1669] Found device 3 with properties: \n",
            "name: NVIDIA A100-SXM4-40GB major: 8 minor: 0 memoryClockRate(GHz): 1.41\n",
            "pciBusID: 0000:4e:00.0\n",
            "2023-08-25 04:48:03.593002: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1669] Found device 4 with properties: \n",
            "name: NVIDIA A100-SXM4-40GB major: 8 minor: 0 memoryClockRate(GHz): 1.41\n",
            "pciBusID: 0000:87:00.0\n",
            "2023-08-25 04:48:03.594250: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1669] Found device 5 with properties: \n",
            "name: NVIDIA A100-SXM4-40GB major: 8 minor: 0 memoryClockRate(GHz): 1.41\n",
            "pciBusID: 0000:90:00.0\n",
            "2023-08-25 04:48:03.595492: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1669] Found device 6 with properties: \n",
            "name: NVIDIA A100-SXM4-40GB major: 8 minor: 0 memoryClockRate(GHz): 1.41\n",
            "pciBusID: 0000:b7:00.0\n",
            "2023-08-25 04:48:03.596752: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1669] Found device 7 with properties: \n",
            "name: NVIDIA A100-SXM4-40GB major: 8 minor: 0 memoryClockRate(GHz): 1.41\n",
            "pciBusID: 0000:bd:00.0\n",
            "2023-08-25 04:48:03.596864: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "2023-08-25 04:48:03.596894: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
            "2023-08-25 04:48:03.596913: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
            "2023-08-25 04:48:03.596931: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
            "2023-08-25 04:48:03.597035: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11\n",
            "2023-08-25 04:48:03.597052: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
            "2023-08-25 04:48:03.597069: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
            "2023-08-25 04:48:03.616662: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1797] Adding visible gpu devices: 0, 1, 2, 3, 4, 5, 6, 7\n",
            "2023-08-25 04:48:03.616706: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "2023-08-25 04:48:03.666065: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1209] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2023-08-25 04:48:03.666159: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1215]      0 1 2 3 4 5 6 7 \n",
            "2023-08-25 04:48:03.666167: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1228] 0:   N Y Y Y Y Y Y Y \n",
            "2023-08-25 04:48:03.666173: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1228] 1:   Y N Y Y Y Y Y Y \n",
            "2023-08-25 04:48:03.666179: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1228] 2:   Y Y N Y Y Y Y Y \n",
            "2023-08-25 04:48:03.666184: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1228] 3:   Y Y Y N Y Y Y Y \n",
            "2023-08-25 04:48:03.666189: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1228] 4:   Y Y Y Y N Y Y Y \n",
            "2023-08-25 04:48:03.666195: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1228] 5:   Y Y Y Y Y N Y Y \n",
            "2023-08-25 04:48:03.666200: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1228] 6:   Y Y Y Y Y Y N Y \n",
            "2023-08-25 04:48:03.666206: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1228] 7:   Y Y Y Y Y Y Y N \n",
            "2023-08-25 04:48:03.678291: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1354] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 35020 MB memory) -> physical GPU (device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:07:00.0, compute capability: 8.0)\n",
            "2023-08-25 04:48:03.680014: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1354] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 38319 MB memory) -> physical GPU (device: 1, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:0f:00.0, compute capability: 8.0)\n",
            "2023-08-25 04:48:03.681549: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1354] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 38319 MB memory) -> physical GPU (device: 2, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:47:00.0, compute capability: 8.0)\n",
            "2023-08-25 04:48:03.683115: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1354] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 38319 MB memory) -> physical GPU (device: 3, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:4e:00.0, compute capability: 8.0)\n",
            "2023-08-25 04:48:03.684646: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1354] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:4 with 38319 MB memory) -> physical GPU (device: 4, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:87:00.0, compute capability: 8.0)\n",
            "2023-08-25 04:48:03.686188: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1354] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:5 with 38319 MB memory) -> physical GPU (device: 5, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:90:00.0, compute capability: 8.0)\n",
            "2023-08-25 04:48:03.687801: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1354] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:6 with 38319 MB memory) -> physical GPU (device: 6, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:b7:00.0, compute capability: 8.0)\n",
            "2023-08-25 04:48:03.689418: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1354] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:7 with 38319 MB memory) -> physical GPU (device: 7, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:bd:00.0, compute capability: 8.0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "  128/40000 [..............................] - ETA: 1:54 - loss: 0.6943 - acc: 0.4688"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-08-25 04:48:04.485691: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "40000/40000 [==============================] - 10s 260us/sample - loss: 0.6918 - acc: 0.5160\n",
            "Epoch 2/100\n",
            "40000/40000 [==============================] - 10s 245us/sample - loss: 0.6514 - acc: 0.6028\n",
            "Epoch 3/100\n",
            "40000/40000 [==============================] - 10s 244us/sample - loss: 0.6253 - acc: 0.6222\n",
            "Epoch 4/100\n",
            "40000/40000 [==============================] - 10s 243us/sample - loss: 0.5919 - acc: 0.6835\n",
            "Epoch 5/100\n",
            "40000/40000 [==============================] - 9s 233us/sample - loss: 0.5160 - acc: 0.6925\n",
            "Epoch 6/100\n",
            "40000/40000 [==============================] - 9s 232us/sample - loss: 0.4943 - acc: 0.7416\n",
            "Epoch 7/100\n",
            "40000/40000 [==============================] - 9s 230us/sample - loss: 0.5585 - acc: 0.6408\n",
            "Epoch 8/100\n",
            "40000/40000 [==============================] - 9s 231us/sample - loss: 0.5169 - acc: 0.7067\n",
            "Epoch 9/100\n",
            "40000/40000 [==============================] - 9s 229us/sample - loss: 0.5338 - acc: 0.6680\n",
            "Epoch 10/100\n",
            "40000/40000 [==============================] - 9s 234us/sample - loss: 0.5335 - acc: 0.6453\n",
            "Epoch 11/100\n",
            "40000/40000 [==============================] - 9s 232us/sample - loss: 0.5099 - acc: 0.6702\n",
            "Epoch 12/100\n",
            "40000/40000 [==============================] - 10s 242us/sample - loss: 0.4827 - acc: 0.6805\n",
            "Epoch 13/100\n",
            "40000/40000 [==============================] - 10s 245us/sample - loss: 0.4260 - acc: 0.7678\n",
            "Epoch 14/100\n",
            "40000/40000 [==============================] - 10s 242us/sample - loss: 0.3360 - acc: 0.8756\n",
            "Epoch 15/100\n",
            "40000/40000 [==============================] - 10s 241us/sample - loss: 0.4812 - acc: 0.6698\n",
            "Epoch 16/100\n",
            "40000/40000 [==============================] - 10s 261us/sample - loss: 0.5296 - acc: 0.7039\n",
            "Epoch 17/100\n",
            "40000/40000 [==============================] - 11s 283us/sample - loss: 0.4942 - acc: 0.7275\n",
            "Epoch 18/100\n",
            "40000/40000 [==============================] - 11s 283us/sample - loss: 0.3000 - acc: 0.8932\n",
            "Epoch 19/100\n",
            "40000/40000 [==============================] - 11s 277us/sample - loss: 0.2529 - acc: 0.9139\n",
            "Epoch 20/100\n",
            "40000/40000 [==============================] - 11s 265us/sample - loss: 0.2293 - acc: 0.9251\n",
            "Epoch 21/100\n",
            "40000/40000 [==============================] - 11s 263us/sample - loss: 0.2421 - acc: 0.9251\n",
            "Epoch 22/100\n",
            "40000/40000 [==============================] - 11s 266us/sample - loss: 0.1822 - acc: 0.9393\n",
            "Epoch 23/100\n",
            "40000/40000 [==============================] - 10s 262us/sample - loss: 0.1802 - acc: 0.9424\n",
            "Epoch 24/100\n",
            "40000/40000 [==============================] - 11s 264us/sample - loss: 0.1499 - acc: 0.9503\n",
            "Epoch 25/100\n",
            "40000/40000 [==============================] - 10s 262us/sample - loss: 0.1223 - acc: 0.9612\n",
            "Epoch 26/100\n",
            "40000/40000 [==============================] - 11s 265us/sample - loss: 0.1188 - acc: 0.9646\n",
            "Epoch 27/100\n",
            "40000/40000 [==============================] - 11s 265us/sample - loss: 0.1065 - acc: 0.9682\n",
            "Epoch 28/100\n",
            "40000/40000 [==============================] - 11s 264us/sample - loss: 0.0893 - acc: 0.9722\n",
            "Epoch 29/100\n",
            "40000/40000 [==============================] - 11s 270us/sample - loss: 0.0756 - acc: 0.9741\n",
            "Epoch 30/100\n",
            "40000/40000 [==============================] - 10s 261us/sample - loss: 0.1914 - acc: 0.9510\n",
            "Epoch 31/100\n",
            "40000/40000 [==============================] - 10s 255us/sample - loss: 0.1673 - acc: 0.9515\n",
            "Epoch 32/100\n",
            "40000/40000 [==============================] - 10s 255us/sample - loss: 0.1151 - acc: 0.9671\n",
            "Epoch 33/100\n",
            "40000/40000 [==============================] - 10s 257us/sample - loss: 0.0917 - acc: 0.9753\n",
            "Epoch 34/100\n",
            "40000/40000 [==============================] - 10s 262us/sample - loss: 0.1919 - acc: 0.9452\n",
            "Epoch 35/100\n",
            "40000/40000 [==============================] - 11s 264us/sample - loss: 0.1025 - acc: 0.9730\n",
            "Epoch 36/100\n",
            "40000/40000 [==============================] - 11s 267us/sample - loss: 0.0880 - acc: 0.9769\n",
            "Epoch 37/100\n",
            "40000/40000 [==============================] - 11s 270us/sample - loss: 0.3855 - acc: 0.8201\n",
            "Epoch 38/100\n",
            "40000/40000 [==============================] - 11s 267us/sample - loss: 0.3449 - acc: 0.8530\n",
            "Epoch 39/100\n",
            "40000/40000 [==============================] - 11s 275us/sample - loss: 0.2574 - acc: 0.9159\n",
            "Epoch 40/100\n",
            "40000/40000 [==============================] - 11s 270us/sample - loss: 0.3763 - acc: 0.8854\n",
            "Epoch 41/100\n",
            "40000/40000 [==============================] - 11s 267us/sample - loss: 0.3082 - acc: 0.8931\n",
            "Epoch 42/100\n",
            "40000/40000 [==============================] - 11s 267us/sample - loss: 0.2365 - acc: 0.9257\n",
            "Epoch 43/100\n",
            "40000/40000 [==============================] - 11s 271us/sample - loss: 0.1645 - acc: 0.9499\n",
            "Epoch 44/100\n",
            "40000/40000 [==============================] - 11s 265us/sample - loss: 0.1740 - acc: 0.9459\n",
            "Epoch 45/100\n",
            "40000/40000 [==============================] - 11s 266us/sample - loss: 0.1163 - acc: 0.9613\n",
            "Epoch 46/100\n",
            "40000/40000 [==============================] - 11s 271us/sample - loss: 0.4895 - acc: 0.8325\n",
            "Epoch 47/100\n",
            "40000/40000 [==============================] - 11s 266us/sample - loss: 0.5316 - acc: 0.6532\n",
            "Epoch 48/100\n",
            "40000/40000 [==============================] - 10s 261us/sample - loss: 0.5073 - acc: 0.6689\n",
            "Epoch 49/100\n",
            "40000/40000 [==============================] - 10s 259us/sample - loss: 0.4919 - acc: 0.6780\n",
            "Epoch 50/100\n",
            "40000/40000 [==============================] - 11s 264us/sample - loss: 0.4821 - acc: 0.6902\n",
            "Epoch 51/100\n",
            "40000/40000 [==============================] - 11s 264us/sample - loss: 0.4833 - acc: 0.6772\n",
            "Epoch 52/100\n",
            "40000/40000 [==============================] - 11s 263us/sample - loss: 0.4700 - acc: 0.6895\n",
            "Epoch 53/100\n",
            "40000/40000 [==============================] - 11s 264us/sample - loss: 0.4577 - acc: 0.6992\n",
            "Epoch 54/100\n",
            "40000/40000 [==============================] - 11s 264us/sample - loss: 0.4119 - acc: 0.7552\n",
            "Epoch 55/100\n",
            "40000/40000 [==============================] - 11s 264us/sample - loss: 0.2843 - acc: 0.9025\n",
            "Epoch 56/100\n",
            "40000/40000 [==============================] - 11s 266us/sample - loss: 0.2114 - acc: 0.9350\n",
            "Epoch 57/100\n",
            "40000/40000 [==============================] - 11s 270us/sample - loss: 0.1930 - acc: 0.9429\n",
            "Epoch 58/100\n",
            "40000/40000 [==============================] - 11s 266us/sample - loss: 0.1483 - acc: 0.9570\n",
            "Epoch 59/100\n",
            "40000/40000 [==============================] - 10s 262us/sample - loss: 0.1280 - acc: 0.9612\n",
            "Epoch 60/100\n",
            "40000/40000 [==============================] - 11s 265us/sample - loss: 0.2152 - acc: 0.9360\n",
            "Epoch 61/100\n",
            "40000/40000 [==============================] - 11s 269us/sample - loss: 0.1309 - acc: 0.9596\n",
            "Epoch 62/100\n",
            "40000/40000 [==============================] - 11s 267us/sample - loss: 0.1112 - acc: 0.9657\n",
            "Epoch 63/100\n",
            "40000/40000 [==============================] - 11s 266us/sample - loss: 0.1430 - acc: 0.9600\n",
            "Epoch 64/100\n",
            "40000/40000 [==============================] - 11s 268us/sample - loss: 0.1469 - acc: 0.9636\n",
            "Epoch 65/100\n",
            "40000/40000 [==============================] - 11s 276us/sample - loss: 0.2141 - acc: 0.9325\n",
            "Epoch 66/100\n",
            "40000/40000 [==============================] - 11s 277us/sample - loss: 0.1240 - acc: 0.9656\n",
            "Epoch 67/100\n",
            "40000/40000 [==============================] - 11s 280us/sample - loss: 0.1004 - acc: 0.9699\n",
            "Epoch 68/100\n",
            "40000/40000 [==============================] - 11s 266us/sample - loss: 0.0920 - acc: 0.9706\n",
            "Epoch 69/100\n",
            "40000/40000 [==============================] - 11s 270us/sample - loss: 0.1405 - acc: 0.9597\n",
            "Epoch 70/100\n",
            "40000/40000 [==============================] - 11s 273us/sample - loss: 0.1254 - acc: 0.9635\n",
            "Epoch 71/100\n",
            "40000/40000 [==============================] - 11s 277us/sample - loss: 0.1855 - acc: 0.9301\n",
            "Epoch 72/100\n",
            "40000/40000 [==============================] - 11s 275us/sample - loss: 0.2176 - acc: 0.9253\n",
            "Epoch 73/100\n",
            "40000/40000 [==============================] - 11s 269us/sample - loss: 0.1289 - acc: 0.9658\n",
            "Epoch 74/100\n",
            "40000/40000 [==============================] - 11s 272us/sample - loss: 0.1274 - acc: 0.9642\n",
            "Epoch 75/100\n",
            "40000/40000 [==============================] - 11s 267us/sample - loss: 0.2089 - acc: 0.9307\n",
            "Epoch 76/100\n",
            "40000/40000 [==============================] - 11s 268us/sample - loss: 0.1327 - acc: 0.9650\n",
            "Epoch 77/100\n",
            "40000/40000 [==============================] - 11s 272us/sample - loss: 0.1141 - acc: 0.9711\n",
            "Epoch 78/100\n",
            "40000/40000 [==============================] - 11s 264us/sample - loss: 0.0958 - acc: 0.9752\n",
            "Epoch 79/100\n",
            "40000/40000 [==============================] - 10s 261us/sample - loss: 0.0734 - acc: 0.9799\n",
            "Epoch 80/100\n",
            "40000/40000 [==============================] - 11s 266us/sample - loss: 0.0671 - acc: 0.9799\n",
            "Epoch 81/100\n",
            "40000/40000 [==============================] - 10s 260us/sample - loss: 0.0648 - acc: 0.9830\n",
            "Epoch 82/100\n",
            "40000/40000 [==============================] - 10s 259us/sample - loss: 0.1007 - acc: 0.9723\n",
            "Epoch 83/100\n",
            "40000/40000 [==============================] - 10s 257us/sample - loss: 0.1541 - acc: 0.9572\n",
            "Epoch 84/100\n",
            "40000/40000 [==============================] - 10s 262us/sample - loss: 0.1167 - acc: 0.9712\n",
            "Epoch 85/100\n",
            "40000/40000 [==============================] - 10s 250us/sample - loss: 0.0823 - acc: 0.9802\n",
            "Epoch 86/100\n",
            "40000/40000 [==============================] - 10s 254us/sample - loss: 0.0780 - acc: 0.9810\n",
            "Epoch 87/100\n",
            "40000/40000 [==============================] - 10s 260us/sample - loss: 0.2230 - acc: 0.9251\n",
            "Epoch 88/100\n",
            "40000/40000 [==============================] - 11s 265us/sample - loss: 0.1594 - acc: 0.9488\n",
            "Epoch 89/100\n",
            "40000/40000 [==============================] - 10s 261us/sample - loss: 0.0776 - acc: 0.9716\n",
            "Epoch 90/100\n",
            "40000/40000 [==============================] - 10s 262us/sample - loss: 0.0623 - acc: 0.9818\n",
            "Epoch 91/100\n",
            "40000/40000 [==============================] - 10s 262us/sample - loss: 0.0689 - acc: 0.9768\n",
            "Epoch 92/100\n",
            "40000/40000 [==============================] - 10s 258us/sample - loss: 0.0840 - acc: 0.9767\n",
            "Epoch 93/100\n",
            "40000/40000 [==============================] - 10s 257us/sample - loss: 0.0656 - acc: 0.9818\n",
            "Epoch 94/100\n",
            "40000/40000 [==============================] - 11s 267us/sample - loss: 0.0584 - acc: 0.9822\n",
            "Epoch 95/100\n",
            "40000/40000 [==============================] - 11s 267us/sample - loss: 0.0707 - acc: 0.9793\n",
            "Epoch 96/100\n",
            "40000/40000 [==============================] - 11s 270us/sample - loss: 0.2608 - acc: 0.9010\n",
            "Epoch 97/100\n",
            "40000/40000 [==============================] - 11s 269us/sample - loss: 0.3499 - acc: 0.8037\n",
            "Epoch 98/100\n",
            "40000/40000 [==============================] - 11s 273us/sample - loss: 0.1663 - acc: 0.9535\n",
            "Epoch 99/100\n",
            "40000/40000 [==============================] - 11s 271us/sample - loss: 0.2078 - acc: 0.9373\n",
            "Epoch 100/100\n",
            "40000/40000 [==============================] - 11s 267us/sample - loss: 0.1778 - acc: 0.9488\n",
            "10000/10000 [==============================] - 3s 344us/sample - loss: 0.9657 - acc: 0.7580\n"
          ]
        }
      ],
      "source": [
        "history = rnn.fit(x_train,y_train,epochs = 100,batch_size=128,verbose = 1)\n",
        "score = rnn.evaluate(x_test, y_test, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blwOshLD0FQb",
        "outputId": "6b6d92f1-3a33-425d-8f06-166ebba7e81d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Score: 0.9657272222042084\n",
            "Test Accuracy: 0.758\n"
          ]
        }
      ],
      "source": [
        "print(\"Test Score:\", score[0])\n",
        "print(\"Test Accuracy:\", score[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGQ5CGWd7pLX"
      },
      "outputs": [],
      "source": [
        "model = rnn.save('rnn.h5')\n",
        "loaded_model = load_model('rnn.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fKgbBP4Flqd"
      },
      "source": [
        "\n",
        "\n",
        "#**Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6nkcsHsvMyI",
        "outputId": "010c16f6-119a-4e7a-9260-2b079c99754c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[9.38848555e-01]\n",
            " [1.00796148e-01]\n",
            " [5.69291227e-03]\n",
            " ...\n",
            " [1.00878276e-01]\n",
            " [5.97474049e-04]\n",
            " [1.97202727e-01]]\n",
            "23950    1\n",
            "3965     0\n",
            "34102    1\n",
            "19971    1\n",
            "48034    1\n",
            "        ..\n",
            "24569    0\n",
            "2404     0\n",
            "14261    0\n",
            "4451     0\n",
            "28786    1\n",
            "Name: sentiment, Length: 10000, dtype: int64\n",
            "Correct Prediction: 7580\n",
            "Wrong Prediction: 2420\n",
            "Accuracy: 75.8\n"
          ]
        }
      ],
      "source": [
        "y_pred = rnn.predict(x_test, batch_size = 128)\n",
        "print(y_pred)\n",
        "print(y_test)\n",
        "for i in range(len(y_pred)):\n",
        "  if y_pred[i]>0.5:\n",
        "    y_pred[i] = 1\n",
        "  else:\n",
        "    y_pred[i] = 0\n",
        "\n",
        "true = 0\n",
        "for i, y in enumerate(y_test):\n",
        "    if y == y_pred[i]:\n",
        "        true += 1\n",
        "\n",
        "print('Correct Prediction: {}'.format(true))\n",
        "print('Wrong Prediction: {}'.format(len(y_pred) - true))\n",
        "print('Accuracy: {}'.format(true/len(y_pred)*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xb1KdQ_pNJIW"
      },
      "source": [
        "Message: **Nothing was typical about this. Everything was beautifully done in this movie, the story, the flow, the scenario, everything. I highly recommend it for mystery lovers, for anyone who wants to watch a good movie!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yN_6mY24Jlw"
      },
      "source": [
        "#**Example review**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_lSReZWvapL",
        "outputId": "fef7b827-f0d5-4d64-ef29-694e5246c61b"
      },
      "outputs": [
        {
          "name": "stdin",
          "output_type": "stream",
          "text": [
            "Movie Review:  movie is good\n"
          ]
        }
      ],
      "source": [
        "review = str(input('Movie Review: '))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VTkE6KF4P31"
      },
      "source": [
        "#**Pre-processing of entered review**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-isbUeevaq5",
        "outputId": "2cabbed4-e13b-46bd-c5f1-48337016e4ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cleaned:  movie is good\n",
            "Filtered:  ['movie good']\n"
          ]
        }
      ],
      "source": [
        "# Pre-process input\n",
        "regex = re.compile(r'[^a-zA-Z\\s]')\n",
        "review = regex.sub('', review)\n",
        "print('Cleaned: ', review)\n",
        "\n",
        "words = review.split(' ')\n",
        "filtered = [w for w in words if w not in english_stops]\n",
        "filtered = ' '.join(filtered)\n",
        "filtered = [filtered.lower()]\n",
        "\n",
        "print('Filtered: ', filtered)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Te3OyQohvaua",
        "outputId": "fe68a30b-07a4-4ea0-fabb-67edfde34492"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[3 9 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
          ]
        }
      ],
      "source": [
        "tokenize_words = token.texts_to_sequences(filtered)\n",
        "tokenize_words = pad_sequences(tokenize_words, maxlen=max_length, padding='post', truncating='post')\n",
        "print(tokenize_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9X_Haz8g4U7Q"
      },
      "source": [
        "#**Prediction**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVWAZXuZviIp",
        "outputId": "25403922-c01c-41f1-f21a-1480fc8a320a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.9390594]]\n"
          ]
        }
      ],
      "source": [
        "result = rnn.predict(tokenize_words)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVgQvGXBviME",
        "outputId": "5077b5fd-fe76-4acd-82f9-68dcffcb79be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "positive\n"
          ]
        }
      ],
      "source": [
        "if result >= 0.7:\n",
        "    print('positive')\n",
        "else:\n",
        "    print('negative')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8obEBRbMffIk"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}